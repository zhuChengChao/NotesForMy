# 机器学习：聚类算法

> 聚类就是对大量**未知标注的数据集**，按照数据**内部存在的数据特征**将数据集划分为多个不同的类别，使类别内的数据比较相似，类别之间的数据相似度比较小；**属于无监督学习**。
>
> 聚类算法的重点是计算样本项之间的**相似度**，有时候也称为样本间的**距离**。

## 相似度/距离

### 闵可夫斯基距离

$$
dist(X,Y)=\quad\sqrt[p]{\sum_{i=1}^{n}{|x_i-y_i|^p}}
$$

$$
其中X=(x_1,x_2,...,x_n),Y=(y_1,y_2,...,y_n)
$$

* 当p是1的时候为哈曼顿距离(Manhattan)/城市距离：

  * $$
    dist(X,Y)=\sum_{i=1}^{n}{|x_i-y_i|}
    $$

* 当p为2的时候为欧式距离(Euclidean)：

  * $$
    E\_dist(X,Y)=\sqrt{\sum_{i=1}^{n}{|x_i-y_i|^2}}
    $$

* 当p为无穷大的时候是切比雪夫距离(Chebyshew)：

  * $$
    C\_dist(X,Y)=max(|x_i-y_i|)
    $$

  > 有数学性质可得，当p为无穷大时，有如下：
  > $$
  > dist(X,Y)=\quad\sqrt[p]{\sum_{i=1}^{n}{|x_i-y_i|^p}}
  > $$
  >
  > $$
  > \leq\quad\sqrt[p]{n\times max(|x_i-y_i|^p)}=max(|x_i-y_i|)\times\quad\sqrt[p]{n}
  > $$
  >
  > $$
  > ≈max(|x_i-y_i|)
  > $$
  >
  > 

### 标准化欧氏距离

标准化欧氏距离是针对欧氏距离的缺点而作的一种改进，即将数据的各维分量都“标准化”到均值、方差相等，即均值为0方差为1 。
$$
现有两个n维向量：a(x_{11}, x_{12}, ...,x_{1n}),b(x_{21}, x_{22}, ...,x_{2n})
$$

$$
使各维数据满足标志正态分布：x_i^{′}=\frac{x_i-\overline{x_i}}{s_i},i表示维度
$$

$$
欧式距离计算：dist_{12}=\sqrt{\sum_{i=1}^{n}{(\frac{x_{1i}-x_{2i}}{s_i})^2}}
$$

### 夹角余弦相似度

通过计算两个向量的夹角余弦值来评估他们的相似度。

有两个n维向量如下：
$$
a(x_{11}, x_{12}, ...,x_{1n}),b(x_{21}, x_{22}, ...,x_{2n})
$$

$$
cos(θ)=\frac{\sum_{k=1}^{n}{x_{1k}x_{2k}}}{\sqrt{\sum_{k=1}^{n}{x_{1k}^2}}\times\sqrt{\sum_{k=1}^{n}{x_{2k}^2}}}=\frac{a^T\cdot b}{|a||b|}
$$

### KL距离

KL距离，也叫做相对熵。它衡量的是相同事件空间里的两个概率分布的差异情况，计算公式如下：
$$
D(P||Q)=\sum_x{P(x)log \frac{P(x)}{Q(x)}}
$$
[参考：KL距离](https://www.cnblogs.com/nlpowen/p/3620470.html)

### Jaccard相似系数

杰卡德相似系数：用于比较有限样本集之间的相似性与差异性，Jaccard系数值越大，样本相似度越高，计算公式如下：
$$
J(A,B)=\frac{|A∩B|}{|A∪B|}
$$

$$
dist(A,B)=1-J(A,B)=\frac{|A∪B|-|A∩B|}{|A∪B|}
$$

### Pearson相关系数

Pearson相关系数是用来衡量两个数据集合是否在一条线上面，它用来衡量定距变量间的线性关系；

计算公式如下：
$$
ρ_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}=\frac{E[(X-E(X))(Y-E(Y))]}{\sqrt{D(X)} \sqrt{D(Y)}}
$$

$$
=\frac{\sum_i^n{(X_i-μ_X)(Y_i-μ_Y)}}{\sqrt{\sum_i^n{(X_i-μ_X)^2}}\times\sqrt{\sum_i^n{(Y_i-μ_Y)^2}}}
$$

$$
dist(X,Y) = 1-ρ_{XY}
$$

## 聚类

**基本思想：**

* 根据具有M个对象的数据集，构件一个具有k个簇的模型，合理的将这M个对象进行划分，其中满足如下条件：
  * 每个簇中至少包含一个对象；
  * 每个对象有且仅属于一个簇；
* 大致流程：对于给定的类别数目k，首先给定初始划分，通过迭代改变样本和簇的隶属关系，使的每次处理后得到的划分方式比上一次的好。

### K-means

#### 算法流程

k-means作为聚类的最广泛，最基础的算法，大致流程如下：

假设有输入样本：
$$
T=X_1,X_2,...,X_m
$$

1. 随机选择K个样本作为初始聚类中心，例如：

$$
K个簇中心：a_1, a_2, ..., a_k
$$

$$
每个簇样本数量：N_1, N_2,...,N_k
$$

2. 对于每个样本，从上述K个聚类中心中寻找距离最近的一个，并且把该数据标记为属于该聚类；

3. 在所有的样本都被标记完成后，根据每个聚类中的样本，更新聚类中心；

> - 使用MSE作为目标函数时，如下：
>
> $$
> J(a_1, a_2, ..., a_k)=\frac{1}{2}\sum_{j=1}^{K}{\sum_{i=1}^{N_j}{(\vec{x_i}-\vec{a_j})^2}}
> $$
>
> - 要获取最优解，也就是使得目标函数要尽可能的小，对J求偏导，可得簇中心点a 的更新公式为：
>
> $$
> \frac{∂J}{∂a_j}=\frac{1}{2}\frac{∂\sum_{i=1}^{N_j}{(\vec{x_i}-\vec{a_j})^2}}{∂a_j}
> $$
>
> $$
> =-\sum_{i=1}^{N_j}{(\vec{x_i}-\vec{a_j})}
> $$
>
> $$
> 令上式为0，得\vec{a_j}=\frac{1}{N_j}\sum_{i=1}^{N_j}{\vec{x_i}}
> $$

4. 重复2,3两步操作，知道达到某个终止条件

终止条件：迭代次数、MSE、簇中心点的变化率

#### 存在问题

* K-means在迭代过程中使用所有点的均值作为新的中心点，若簇中存在**异常值**，将导致均值出现偏差

  > 可通过K-Mediods聚类(K中值聚类)解决

* K-means存在**初值敏感**问题，选择不同的初始值可能会导致最终划分的簇不同

#### 优点

* 容易理解，效果还行；
* 处理大数据集时，该算法可以保证较好的伸缩性和高效率；
* 当簇近似高斯分布的时候，效果很好。

### 二分K-Means算法

二分K-Means算法是一种弱化初始质心的一种算法，一定程度上解决了K-Means算法对初始簇心比较敏感的问题。

#### 思路：

1. 将所有样本数据作为一个簇放到一个队列中；

2. 从队列中选择一个簇进行K-means算法划分，划分为两个子簇后添加到队列中；

3. 继续第2步操作，直到达到终止条件(聚簇数量、MSE、迭代次数)
4. 队列中的簇就是最终的分类簇集合

#### 划分方式：

从队列中选择划分聚簇的规则一般有以下两种方式：

* 选择样本数量最多的簇进行划分
* 对所有的簇计算误差和SSE(SSE簇内误差平方和，也可以认为是距离函数的一种变种)，选择SSE最大的居村进行划分操作(优先选择这策略)

$$
SSE=\sum_{i=1}^{n}{w_i(\vec{x_i}-\vec{a_{x_{ui}}})}
$$

### K-means++

也是为了解决K-means算法对初始簇心敏感问题，K-means++在选择初始的K个簇心时，并不是和K-means一样随机选择，而是采用以下步骤进行选择：

1. 从数据集中任选一个节点作为第一个聚类的中心；
2. 对数据集中的每个样本点，计算x到所有已确定的聚类簇心点的距离D(X)，基于D(X)采用线性概率选择出下一个聚类中心点(距离较远的一个点成为新增的一个聚类中心点)；
3. 重复2步骤，直到选择出K个聚类中心点。

**缺点**：

k-means++ 最主要的缺点在于其内在的顺序执行特性，得到 k 个聚类中心必须遍历数据集 k 次，并且当前聚类中心的计算依赖于前面得到的所有聚类中心，**这使得算法无法并行扩展**，极大地限制了算法在大规模数据集上的应用。

### K-means||

解决了K-means++算法的缺点，其主要思路如下：

* 改变了每次遍历时的取样规则；
* 并非按照K-means++算法每次在遍历时只获取一个样本，而是**每次获取K个样本**；
* 重复取样操作logn次，可得到k×logn个样本点组成的集合；
* 然后再将这些抽样出来的样本聚类出K个点；
* 最后用这K个点作为K-means的初始簇心。

> 实践证明，一般5此重复采样就可以保证一个比较好的聚簇中心点

### Canopy

Canopy算法为一种相对“粗”的聚类算法，执行速度快，但进度较低。

#### 执行步骤

1. 给定样本列表$L=x_1,x_2,...,x_m$，和**先验值**$r_1,r_2(r_1>r_2)$;

2. 从列表L中选择一节点P，计算P到所有簇中心的距离，并选择出最小距离$D(P, a_j)$；

   > 若此时还不存在簇中心，则该P点为一个新的簇

3. 若距离D小于$r_1$，表示该节点属于该聚簇，添加到该聚簇列表中；

4. 若距离D小于$r_2$，表示该节点不仅仅属于该聚簇，还表示和当前聚簇中心非常近，所以将该聚的中心点设置为该簇中所有样本的中心点，并将P从列表L中删除；

5. 如距离D大于$r_1$，那么该节点P形成一个新的聚簇；

6. 一直迭代到列表L中的元素数据不再有变化/元素个数为0的时候，结束循环操作。

> Canopy算法最终得到的结果，聚簇之间是可能存在重叠的，但是不存在某个对象不属于任何聚簇的情况

#### **应用场景**

由于K-means存在初始聚簇中心敏感问题，因此常使用Canopy+K-means算法混合形式进行模型构建 

* 先使用Canopy算法进行“粗”聚类，得到K个聚类中心点；
* K-means使用Canopy得到的K个聚类中心点作为初始的中心点，进行“细”聚类

此方法的优点如下：

* 执行速度快(先进行了一次聚簇中心点选择的预处理)；
* 不需要给定K值，应用场景多；
* 能缓解K-means算法对于初始聚类中心敏感问题。

### Mini Batch K-means

该算法是K-means的一种变种，采用了mini-batch的方式，即每次训练时使用的数据集是从样本集中随机抽取出来的子集，此方式可以减少计算的时间与收敛时间，而最终的效果只是略差于标准的K-means算法。

#### 执行步骤

1. 抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型；
2. 继续在训练集中抽取部分样本数据，添加到模型中，进行分配；
3. 更新聚簇中心；
4. 循环迭代，知道中心点稳定或达到迭代次数，停止计算操作。

## 聚类衡量指标

### 均一性

一个簇中只包含一个类别的样本，则满足均一性；可认为是**正确率**(每个聚簇中正确分类的样本**占据该聚簇总样本数的比例**)；
$$
p=\frac{1}{K}\sum_{k=1}^{K}{\frac{N(C_k==K_k)}{N(K_k)}}
$$

### 完整性

同类别样本被归类到相同簇总，满足完整性；每个聚簇中正确分类的样本数**占该类型的总样本数比例和**
$$
r=\frac{1}{K}\sum_{k=1}^{K}{\frac{N(C_k==K_k)}{N(C_k)}}
$$

### V-measure

**均一性**和**完整性**的加权平均
$$
v_\beta=\frac{(1+\beta^2)\cdot pr}{\beta^2\cdot p+r}
$$

### 轮廓系数

#### 簇内不相似度

计算样本$X_i$到同簇其他样本的平均距离，令其为$a_i$，该值越小，表示该样本越应该被聚类到该簇中；簇$C_k$中的所有样本的$a_i$均值被称为簇$C_k$的簇内不相似度。

#### 簇间不相似度

计算样本$X_i$到其他簇样本的平均距离，记为$b_ik$，去除最小值：
$$
b_i=min(b_{i1}, b_{i2}, ..., b_{iK})
$$
当$b_i$越大，表示样本$X_i$越不属于其他簇。

#### 轮廓系数

$$
s(i) = \frac{b_i-a_i}{max(a_i,b_i)}
$$

$$
=\begin{cases} 1-\frac{a_i}{b_i}，a_i<b_i\\ 0， a_i=b_i\\ \frac{a_i}{b_i}-1，a_i>b_i\end{cases}
$$

可得当$s(i)$的值越接近1表示样本i聚类越合理；越接近-1，表示样本$X_i$应该被分类到其他的簇中；近似为0，说明$X_i$在边界上；

所有样本的$s(i)$的均值为聚类结果的轮廓系数

> 其他的衡量指标：
>
> * 调整兰德系数(ARI) ，不常见，略
> * 调整互信息(AMI) ，不常见，略

## 层次聚类

层次聚类对于给定的数据集进行层次分解，直到满足某种条件为止，传统层次聚类主要分为两大类算法：

### AGNES

AGNES，agglomerative NESting，凝聚的层次聚类；采用**自底向上**的策略；

> agglomerative：凝聚
>
> nesting：嵌套

* 将每个对象作为一个簇；
* 根据**某些准则**(两个簇之间的相似度度量方式)，将簇一步步合并；
* 反复进行合并过程，直到满足终止条件(簇的数目)。

#### 某些准则

在AGNES算法中，在合并簇时，依靠的是某些准则来计算簇间的距离，有如下几种方式：

* **最小距离(SL聚类)**：
  * 两个聚簇中最近的两个样本之间的距离(single/word-linkage聚类法)
  * 最终得到的模型容易形成**链式结构**
* **最大距离(CL聚类)**：
  * 两个聚簇中最远的两个样本之间的距离(complete-linkage聚类法)
  * 如果存在异常值，那么结构可能不太稳定
* **平均距离(AL聚类)**：
  * 两个聚簇中样本间两两距离的平均值(average-linkage聚类法)
  * 两个聚簇中样本间两两距离的中值(median-linkage聚类法)

### DIANA

DIANA，DIvisive ANALysis，分裂的层次聚类，采用**自顶向下**的策略；

> divisive：分裂
>
> analysis：分析

* 将所有的对象置于一个簇中；
* 按照**某种规则**逐渐细分为越来越小的簇；
* 直到达到某个终止条件。

#### 某种规则

* 各自簇的误差；
* 各自簇的SSE；
* 选择样本数量最多的簇；

> 和二分K-Means算法中的划分依据相同，可参考上方相应章节。

### AGNES和DIANA特点

* 简单，容易理解；

* 合并点/分裂点选择不容易；

* 合并/分类的操作不能进行撤销；

* 大数据集不太适合：执行效率较低，其时间复杂度为：$O(t \times n^w)$

  > [时间复杂度O(n)](https://www.zybang.com/question/072f79c7d8efa88fd8808216cf7ae010.html)

### BIRCH

brich：平衡迭代消减聚类法，聚类特征使用**3元祖**表示一个簇的相关信息，通过构建满足**分支因子**和**类直径**限制的聚类特征树来求聚类；

* 3元祖：(N，LS，SS)，其中N代表这个簇中拥有的样本点的数量；LS代表了这个簇中拥有的样本点各特征维度的和(向量)，SS代表了这个簇中拥有的样本点各特征维度的平方和(向量)

* 分支因子：规定了树的每个节点的最大子节点数，超过后需要进行分裂；
* 类直径：每个簇直径的最大阈值，当超过后，则作为一个新的簇；

**特点：**

* 适合大规模数据集，线性效率；

> 树的构建是动态过程，可随时根据数据对模型进行更新

* 只适合分布呈凸形或者球形的数据集，需要给定簇之间的相关参数(分支因子和类直径)，聚类个数可给可不给
  * 当给定聚类个数时：先生成树，根据聚类个数合并叶子节点；
  * 未给定聚类个数时：直接生成树

### CURE

Clustering Using Representatives，使用代表点的聚类法，该算法先把每个数据点看成一个簇，然后合并距离最近的簇直到簇的个数满足要求；这似乎和AGNES的算法相同，当然存在区别：

AGNES：使用所有点/中心点+距离代表一个簇；

**CURE：**从簇中选择固定的数目的，分布较好的最能代表该簇的点来作为此类簇的代表，并将这些代表点乘上**收缩因子**，使其更加靠近簇中心；

#### 特点

* 每类簇有多于一个的代表点使得模型可以匹配那些非球形的场景；

* 收缩因子的使用可以减少噪音对聚类的影响 ；

* 针对大型数据库，CURE采用随机取样和划分两种方法的结合：一个随机样本首先被划分，每个划分被部分聚类。

> 稍作了解：）

## 密度聚类

密度聚类思想：只要样本点的密度大于某个阈值，则将该样本添加到最近的簇中。

特点：

* 这类算法可以克服基于距离的算法只能发现凸聚类的缺点，**可以发现任意形状的聚类**，而且对噪声数据不敏感；
* 但是计算复杂度搞，计算量大。

### DBSCAN

DBSCAN，Density-Based Spatial Clustering of Applications with Noise，一个比较有代表性的基于密度的聚类算法，相比于基于划分的聚类方法和层次聚类方法，DBSCAN算法将簇定义为**密度相连的点的最大集合**，能够将足够高密度的区域划分为簇，并且在具有噪声的空间数据上能够发现任意形状的簇。

**核心思想：**

​	**用一个点的ε领域内的邻居点数衡量该点所在空间的密度**，该算法可以找出形状不规则的簇，且不需要事先指定好簇的数量。

#### 一些概念

* ε领域：给定对象在半径ε内的区域

$$
N_ε(x)=\{y∈X:dist(x,y)\leq ε \}，X样本集
$$

* 密度(density)：ε领域中x的密度，是一个整数值，依赖于半径ε

$$
p(x)=|N_ε(x)|
$$

* 阈值M：定义核心点时的阈值
* 核心点(core point)：对应于稠密区域内部的点，如下

$$
若p(x) \geq M,则称x为X的核心点；
$$

$$
记由X中所有核心点构成的集合为X_c;记X_{nc}=X-X_c表示由X中所有非核心点构成的集合
$$

* 边界点(border point)：若非核心点x的领域ε中**存在**核心点，那么认为x为X的边界点；由X中所有边界点构成的集合为$X_{bd}$；即，边界点对应于稠密区域边缘的点；

$$
x∈X_{nc};∃y∈X;y∈N_ε(x)∩X_c
$$

* 噪声点(noise point)：集合中除了边界点和核心点之外的点都为噪声点，所以的噪声点组成的集合为$X_{noi}$，即，噪声点对应稀疏区域的点；
  $$
  X_{noi}=X-(X_c∪X_{bd})
  $$

* 直接密度可达(directly density-reachable)：给定一个对象集合X，如果y是在x的ε领域内，且x为核心对象，那么可以说对象y从对象x出发时直接密度可达的；

$$
x,y∈X;x∈X_c,y∈N_ε(x)
$$

* 密度可达(density-reachable)：如果存在一个对象链$p_1,p_2,...,p_m$，如果满足$p_{i+1}$是从$p_i$直接密度可达的，那么称$p_m$是从$p_1$密度可达的；
* 密度相连(density-connected)：在集合X中，若存在一个对象o，使得对象x和y均由o密度可达，则称x与y密度相连
* 簇(cluster)：一个基于密度的簇是最大的密度相连对象的集合C；满足以下两个条件：
  * 最大性(Maximality)：若x属于C，且y是从x密度可达的，那么y也属于C；
  * 连接性(Connectivity)：若x属于C，y也属于C，则x和y是密度相连的。

#### 算法流程

* 若一个点x的ε领域包含多于m个对象，则创建一个x作为核心对象的新簇；
* 寻找并合并核心对象直接密度可达的对象；
* 没有新点可以更新簇的时候，算法结束。

#### 算法特征描述

* 每个簇至少包含一个核心对象；
* 非核心对象可以是簇的一部分，构成簇的边缘；
* 包含过少对象的簇被认为是噪声

#### 优缺点

* 优点：
  * 不需要事先给定cluster的数目；
  * 可以发现任意形状的cluster；
  * 能够找出数据中的噪音，且对噪音不敏感；
  * 算法只需要两个输入参数，ε领域值和m阈值；
  * 聚类结果几乎不依赖节点的遍历顺序；
* 缺点：
  * DBSCAN算法聚类效果依赖距离公式的选取，最常用的距离公式为欧几里得距离；但是对于高维数据，由于维数太多，距离的度量已经变的不是那么重要；
  * DBSCAN算法不适合数据集中密度差异很小的情况。

### 密度最大值算法

MDCA(Maximum Density Clustering Application)算法基于**密度的思想**引入划分聚类中，使用密度而不是初始点作为考察簇归属情况的依据，能够自动确定簇数量并发现任意形状的簇；另外MDCA一般不保留噪声，因此也避免了阈值选择不当情况下造成的对象丢弃情况。

**基本思路**：寻找**最高密度**的对象和它所在的**稠密区域**；MDCA算法在原理上来讲，和密度的定义没有关系，采用任意一种密度定义公式均可，一般情况下采用DBSCAN算法中的密度定义方式。

#### 基本概念

* 最大密度点：

$$
x_{max}=\{x|x∈X;∀y∈X,density(x)\geq density(y)\}
$$

* 有序序列：根据所有对象与$x_{max}$的距离对数据重新排序

$$
S_{x_{max}}=\{x_1,x_2,...,x_n | dist(x_{max},x_1) \leq dist(x_{max},x_2)\leq ... \leq dist(x_{max},x_n) \}
$$

* 密度阈值$density_0$：当节点的密度值大于密度阈值的时候，认为该节点属于一个比较固定的簇，在第一次构建基本簇的时候，就将这些节点添加到对应簇中，如果小于这个值的时候，暂时认为该节点为噪声节点。

* 簇间距离：对于两个簇$C_1$和$C_2$之间的距离，采用两个簇中最近节点之间的距离作为簇间距离：
  $$
  dist(C_1,C_2)=min(dist(p,q));p∈C_1,q∈C_2
  $$
  
* 聚簇距离阈值$dist_0$：当两个簇的簇间距离小于给定阈值的时候，这两个簇的结果数据会进行合并操作；

- M值：初始簇中最多数据样本的个数

#### 算法步骤

* 将数据集划分为基本簇：
  * 选择数据集中最大密度点$x_{max}$，形成以最大密度点为核心的新簇$C_i$，按照距离排序计算出序列$S_{x_{max}}$，对序列的前M个样本数据进行循环判断，如果节点的密度大于等于$density_0$，那么将当前节点添加到$C_i$中；
  * 循环处理剩下的数据集X，选择最大密度点$X_max$，并构建基本簇$C_{i+1}$，直到X中剩余的样本数据的密度均小于$density_0$
* 使用凝聚层次聚类思想，合并较近的基本簇，得到最终的簇划分：
  * 合并要求：簇间距离小于等于$dist_0$，若无此情况，则不需要合并操作
* 处理剩余点，归入最近的簇
  * 最常用，最简单的方式是：将剩余的样本对象归入到最近的簇

## 谱聚类

[谱聚类(Spectral Clustering)算法介绍](https://blog.csdn.net/qq_24519677/article/details/82291867)

> 挖坑，等有需求了再来好好理解一下 :expressionless:

## 案例代码

* 00_KMeans聚类应用；

* 01_案例一：K-Means算法聚类；

* 02_案例二：K-Means算法和Mini Batch K-Means算法比较；

* 03_案例三：K-Means算法和Mini Batch K-Means算法效果评估；

* 04_案例四：层次聚类(AGNES)算法采用不同距离计算策略导致的数据合并不同形式；

* 05_案例五：层次聚类(BIRCH)算法参数比较；

* 06_案例六：密度聚类(DBSCAN)算法案例；

* 07_案例七：谱聚类(SC)算法案例；

  > 未运行，未理解

* 08_综合案例一：不同聚类算法的比较；

* 09_综合案例二：基于K-Means算法进行图片压缩；

相关代码提交于GitHub，传送门：https://github.com/zhuChengChao/ML-Cluster