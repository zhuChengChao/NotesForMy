# 机器学习：决策树

> 决策树（Decision Tree）是一种简单但是广泛使用的分类器，此处xuewei记录一下个人的学习笔记。

## 前提知识

在进行决策树的了解之前，首先需要对信息熵等概念有一个大致的了解。

### 比特化

* 假设目前有一组随机变量X，各值出现的概率如下：

| A        | B        | C        | D        |
| -------- | -------- | -------- | -------- |
| 00       | 01       | 10       | 11       |
| P(A)=1/4 | P(B)=1/4 | P(C)=1/4 | P(D)=1/4 |

由这一组变量X组成的序列为：CCDBBCA，若将这个序列通过二进制进行传输，则其编码为：101011010100

可见，我们可以使用**两个Bit位**来表示一个随机的变量

* 当变量X出现的概率不同是，如下：

| A        | B        | C        | D        |
| -------- | -------- | -------- | -------- |
| 0        | 10       | 110      | 111      |
| P(A)=1/2 | P(B)=1/4 | P(C)=1/8 | P(D)=1/8 |

此时，描述一个变量不再需要两个比特了，而是**1.75个bit**即可，如下所示：
$$
E = 1\times\frac{1}{2}+2\times\frac{1}{4}+3\times\frac{1}{8}+3\times\frac{1}{8}=1.75
$$

$$
E = -log_2(\frac{1}{2})\times\frac{1}{2}-log_2(\frac{1}{4})\times\frac{1}{4}-log_2(\frac{1}{8})\times\frac{1}{8}-log_2(\frac{1}{8})\times\frac{1}{8}=1.75
$$

* 总结：

假设随机变量X具有m个值 ，分别为: $V_1,V_2,....,V_m$ ，每个值出现的概率如下：

* $P(X=V_1)=p_1$
* $P(X=V_2)=p_2$
* $P(X=V_3)=p_3$
* ... ...
* $P(X=V_m)=p_m$

使用这些变量的期望来表示每个变量需要多少个比特位来描述信息，如下：
$$
E = -p_1log_2(p_1)-p_1log_2(p_1)-...-p_1log_2(p_1)=-\sum_{i=1}^{m}{p_ilog_2(p_i)}
$$

### 信息熵

$$
H(X)=-\sum_{i=1}^{m}{p_ilog_2(p_i)}
$$

* 信息量：一个样本/事件所蕴含的信息，若一个**事件的概率越大**，那么就可以认为**该事件所蕴含的信息越少** ；例如："我很帅"这个事件，不携带任何信息量，因为是一个确定事件；
* 信息熵：1948年，香农引入信息熵；一个系统越是有序，信息熵就越低，一个系统越是混乱，信息熵就越高，所以信息熵被认为是一个系统有序程度的度量；
* **信息熵就是用来描述系统信息量的不确定度**；
* **高信息熵**：表示随机变量X均匀分布，各种情况等概率出现；**低信息熵**：表示随机变量X各种取值不是等概率出现，有的事件概率很大，有的事件概率很小；

* 举个例子：

> 对64个球队进行冠军的预测，若在毫无信息的情况下，最多需要猜测6次
> $$
> log_264=6(bit)
> $$
> 而当你知道了一些信息时，此时可能就不需要进行6次猜测才能猜出谁是冠军了，因为其信息熵改变了；
>
> 香农指出，此时预测的准确的信息量应为：
> $$
> H =  -(p_1lgp_1 + p_2lgp_2 + ... + p_{64}lg_{64})
> $$
> 其中，$p_1,p_2, ......, p_{64}$为队伍夺冠的概率；**H则为信息熵，单位为比特**

### 条件熵

$$
H(Y|X)=\sum_{i=1}^{n}{P(X=v_i)H(Y|X=v_i)}
$$

* 给定条件X的情况下，随机变量Y的信息熵就叫做条件熵
* 举个例子：

| 专业（X） | 性别（Y） |
| --------- | --------- |
| 数学      | M         |
| 计算机    | M         |
| 英语      | F         |
| 数学      | F         |
| 数学      | M         |
| 计算机    | M         |
| 英语      | F         |
| 数学      | F         |

当X为数学时，Y的信息熵如下：
$$
H(Y|X=数学)=H(Y=F|X=数学)+H(Y=M|X=数学)
$$

$$
=-\frac{1}{2}log_2\frac{1}{2}--\frac{1}{2}log_2\frac{1}{2}=1
$$

同理，当X分别计算机和英语时，Y的信息熵如下：
$$
H(Y|X=计算机)=H(Y=F|X=计算机)+H(Y=M|X=计算机)=0-1\log_21=0
$$

$$
H(Y|X=英语)=H(Y=F|X=英语)+H(Y=M|X=英语)=-1\log_21+0=0
$$

由上述，条件熵表示了在已知随机变量X的条件下随机事件
$$
H(Y|X)=p(X=数学)H(Y|X=数学)+p(X=计算机)H(Y|X=计算机)+p(X=英语)H(Y|X=英语)
$$

$$
=\frac{1}{2}*1+\frac{1}{4}*0+\frac{1}{4}*0=0.5
$$

* 同时，条件熵$H(Y|X)$相当于联合熵$H(X,Y)$减去单独的熵$H(X)$，其计算公式如下：

$$
H(Y|X) = H(X,Y)-H(X)
$$

> 详细推导过程如下：
>
> $$
> H(X,Y)=-\sum_{x,y}{p(x,y)log(p(x,y))}
> $$
>
> $$
> =-\sum_{x,y}{p(x,y)log(p(y|x)p(x))}
> $$
>
> $$
> =-\sum_{x,y}{p(x,y)log(p(y|x))}-\sum_{x,y}{p(x,y)log(p(x))}
> $$
>
> $$
> =H(Y|X)-\sum_{x,y}{p(x,y)log(p(x))}
> $$
>
> $$
> =H(Y|X)-\sum_{x}\sum_{y}{p(x,y)log(p(x))}
> $$
>
> $$
> =H(Y|X)-\sum_{x}{log(p(x))}\sum_{y}{p(x,y)}
> $$
>
> $$
> =H(Y|X)-\sum_{x}{log(p(x))p(x)}
> $$
>
> $$
> =H(Y|X)-\sum_{x}{p(x)log(p(x))}
> $$
>
> $$
> =H(Y|X)+H(X)
> $$

> 举个例子：
>
> 比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布H(X,Y)，因为两个事件加起来的**信息量**肯定是大于单一事件的信息量的。假设 H(X)对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知H(X)这个信息量的时候，H(X,Y)剩下的信息量就是条件熵。
>
> 摘录自：https://www.cnblogs.com/kyrieng/p/8694705.html#name2

**总结：**

描述X和Y所需的信息是描述X自己所需的信息,加上给定X的条件下具体化Y所需的额外信息。

### 信息增益

* 信息和消除不确定性是相联系的，此处定义为：**划分数据集之前之后信息发生的变化成为信息增益**；
* **信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度**
* 公式如下：

$$
g(D,A)=H(D)−H(D∣A)
$$

> **根据信息增益的准则，特征选择方法是：对于训练数据集D，计算其中每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征**
>
> 计算流程：
> $$
> H(D)=-\sum_{k=1}^{K}{\frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}}
> $$
>
> $$
> H(D|A)=\sum_{i=1}^{n}{\frac{|D_i|}{|D|}H(D_i)}=-\sum_{i=1}^{n}{\frac{|D_i|}{|D|}\sum_{k=1}^{K}{\frac{|C_{ik}|}{|D_i|}log\frac{|C_{ik}|}{|D_i|}}}
> $$
>
> 将上述结果代入公式，得出信息增益：
> $$
> g(D,A)=H(D)−H(D∣A)
> $$
> 其中
> $$
> D:样本数据集；|D|:样本个数；|C_k|:k类别个数；A：特征A；
> $$
>
> $$
> D_i:根据特征A将总体样本集划分成n个数据集；|D_{ik}|：在划分的第i个数据集中，第k个类别占据的数量
> $$
>
> > 需要注意的是，当在计算熵时，其中的类别的概率由数据估计(即从样本数据)得到时，所定义的熵称为经验熵(empirical entropy)

## 决策树

在有了上述前提知识后，正式进入决策树的内容。

- 决策树模型呈树形结构，其中每个内部节点表示一个属性的测试 ；
- 分类问题中，表示基于特征对实例进行分类的过程，它可以认为是**if-then规则**的集合；
- 通常决策树学习包括三个步骤：**特征选择**、**决策树的生成**和**决策树的修剪**
- 决策树分为两大类：**分类树**和**回归树**，常用算法有ID3、C4.5、CART等 
  
  - 分类树：采用信息增益、信息增益率、基尼系数来评价树的效果，基于概率值进行判断；预测值一般为叶子节点中概率最大类别的作为当前叶子的预测值；
  - 回归树：预测值一般采用叶子节点中所有值的均值累作为当前叶子节点的预测值；一般采用MSE(均方差)来作为树的评价指标；
  
  > $$
  > MSE=\frac{1}{n}\sum_{i=1}^{n}{(y_i-y_{i}^{pred})^2}
  > $$
  

### 决策树的构建

决策树的构建就是进行属性选择，递归的选择最优特征，根据该特征对训练数据进行划分，使得划分的各个子集尽可能的'纯'（让一个分裂子类中待分类的项尽可能的属于同一个类别）

#### 构建步骤

1. 构建根节点，将所有的训练数据都放在根节点，选择一个最优的特征，按这一特征将训练数据按最优的划分方式，划分为不同的子节点；
2. 对上述划分的子节点，再选择出最优的特征以及最优的划分方式，继续划分子节点；
3. 对划分的子节点，继续按照上述2步的方式进行划分，直至满足划分停止的条件

#### 分割属性选择

* 决策树的分割，只是考虑在当前数据特征情况下最好的分割方式，不能进行回溯操作；

* 选择“纯度”越高的特征属性作为当前需要分割的数据集进行分割操作，持续迭代，直到得到最终结果，决策树是通过“纯度”来选择分割特征属性点的 

* 对数据特征量化“纯度“一般有以下三种方式：

  * `Gini`系数

  $$
  Gini=1-\sum_{i=1}^{n}{P_i^2}
  $$

  

  * `Entropy`熵

  $$
  H(X)=-\sum_{i=1}^{n}{p_ilog_2(p_i)}
  $$

  * `Error`错误率

  $$
  Error=1-max_{i=1}^{n}{p_i}
  $$

* 当计算出各个特征属性的量化纯度后，可使用**信息增益**来选择出当前数据集的分割属性：
  
  * 当信息增益越大，说明该在特征属性上损失的"纯度越大"，可将该特征属性放在决策树的上层

#### 停止条件

决策树构建是一个递归的过程，必须给出停止的条件：

* 当每个叶子节点中只有一个类别是会停止；
* 当前叶子节点中的样本数小于某个阈值；

### 决策树效果评估

* 可以通过混淆矩阵来计算准确率、召回率、精确率...

* 可以采用叶子节点的纯度值总和来评估算法的效果，其值越小，效果越好
  $$
  C(T)=\sum_{t=1}^{leaf}{\frac{|D_t|}{|D|}H(t)}
  $$

### 决策树生成算法

决策树的生成主要有以下几种算法：`ID3`，`C4.5`，`CART(Classfication And Regression Tree)`

#### ID3算法

内部使用**信息熵**和**信息增益**来进行构建；每次迭代选择信息增益最大的特征属性作为分隔属性
$$
H(D)=-\sum_{i=1}^{n}{P(i)log_2(P(i))}
$$

$$
Gain(A)=H(D)-H(D|A)
$$

优点：

* 决策树构建速度快；实现简单 

缺点：

* 计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优
* ID3算法不是递增算法 
* ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 
* 抗噪性差 
* 只适合小规模数据集，需要将数据放到内存中 

#### C4.5算法

在ID3算法的基础上提出：

* 使用了**信息增益率**来取代ID3中的**信息增益**；
* 在树的构造中会进行**剪枝**操作；
* 能够自动完成对连续属性的离散化处理；

**信息增益率**
$$
Gain-ratio=\frac{Gain(A)}{H(A)}
$$
优点 :

* 产生的规则易于理解
*  准确率较高
* 实现简单

缺点：

* 对数据集需要进行多次顺序扫描和排序，所以效率较低
* 只适合小规模数据集，需要将数据放到内存中

#### CART算法

使用**基尼系数**作为数据纯度的量化指标来构建的决策树算法就叫做CART(Classification And Regression Tree，分类回归树)算法。

CART算法使用Gini增益作为分割属性选择的标准，选择Gini增益最大的作为当前数据集的分
割属性；可用于分类和回归两类问题。 
$$
Gini=1-\sum_{i=1}^{n}{P(i)^2}
$$

$$
Gain=Gini(D)-Gini(D|A)
$$

**注意事项**：CART算法只能构建二叉树

#### 总结

* ID3和C4.5算法均只适合在小规模数据集上使用 
* ID3和C4.5算法都是单变量决策树
* 当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差
* 决策树分类一般情况只适合小数据量的情况(数据可以放内存) 
* CART算法是三种算法中最常用的一种决策树构建算法
* 三种算法的区别仅仅只是对于当前树的评价标准不同而已，**ID3使用信息增益**、
  **C4.5使用信息增益率**、**CART使用基尼系数**
* CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树

### 决策树优化策略

**剪枝优化**：决策树过渡拟合一般情况是由于节点太多导致的，剪枝优化对决策树的正确率影响是比较
大的，也是最常用的一种优化方式

**随机森林**：利用训练数据随机产生多个决策树，形成一个森林。然后使用这个森林对数据进行预测，
选取最多结果作为预测结果

#### 决策树的剪枝

决策树的剪枝，是决策树算法中最基本、最有用的一种优化方案，主要分为：

* **前置剪枝**：在构建决策树的工程中，提前停止，结果是决策树一般较小；
* **后置剪枝**：在决策树构建好后，对构建好的决策树进行裁剪，一般有以下两种方式：
  * 用单一的叶子节点代替整个子树，叶子节点的分类采用子树中最主要的分类；
  * 将一个子树完全替代另一颗子树

> 前置剪枝的效果没有后置剪枝的效果好，但是后置剪枝存在计算效率问题，存在一定的浪费问题

#### 决策树剪枝过程

对于给定的决策树$T_0$

* 计算所有内部非叶子节点的**剪枝系数**

* 查找**最小剪枝系数**的节点，将其子节点进行删除操作，得到决策树$T_1$

  * 若存在多个最小剪枝系数的节点，选择包含**数据项最多**的节点进行剪枝操作

* 重复上述操作，直到产生的剪枝决策树$T_k$只有一个节点

* 得到决策树$T_0,T_1,T_2,...,T_K$

* 使用**验证集**选择最优子树$T_a$

  * 使用验证集选择最优子树的标准，可以使用原始损失函数来考虑：

  $$
  loss=\sum_{t=1}^{K}{\frac{|D_t|}{|D|}H(t)}
  $$

**最小剪枝系数计算**：

* 叶子节点越多，决策树就越复杂，损失越大；

* 因此对损失函数添加剪枝系数，如下
  $$
  loss_\alpha=loss+\alpha \times n_{leaf}
  $$

* 考虑根节点为r的子树，剪枝后只保留r本身，而删除所有的叶子：

  * 在剪枝前损失函数为：

  $$
  loss_\alpha(R)=loss(R)+\alpha \times n_{leaf}
  $$

  * 在剪枝后损失函数为：

  $$
  loss_\alpha(r)=loss(r)+\alpha \times 1
  $$

* 当剪枝前后损失函数相等时，求得剪枝系数：

$$
\alpha=\frac{loss(r)-loss(R)}{n_{leaf}-1}
$$

## 实际应用

* 决策树案例一：鸢尾花数据分类；
* 决策树案例二：鸢尾花数据特征属性比较；
* 决策树案例三：波士顿房屋租赁价格预测；
* 决策树过拟合和欠拟合；
* 决策树回归模型可视化；
* 决策树分类模型可视化

相关代码提交于GitHub，传送门：https://github.com/zhuChengChao/ML-DecisionTree