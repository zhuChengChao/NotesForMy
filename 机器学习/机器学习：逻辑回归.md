# 机器学习：逻辑回归

> Logistic回归在本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数$g(z)$将上述结果映射到0-1上。

## 理论分析

### 概述

Logistic回归在本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数$g(z)$将上述结果映射到0-1上。

> 优点：计算代价不高，易于理解和实现
>
> 缺点：容易欠拟合，分类精度不高
>
> 适用数据：数值型和标称型

### 映射函数

#### sigmoid

* 函数表示

$$
g(z)=\frac{1}{1+e^{-z}}
$$

$$
z = \theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$

* 导函数：

$$
g^{'}(z)=g(z)(1-g(z))
$$

* 函数图像

![sigmoid](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251744555.png)

> 如下代码：
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> x = np.arange(-10,10,0.1)
> y = 1./(1.+np.exp(-x))
> plt.grid(b=True, ls=':')
> plt.plot(x, y)
> ```

### 逻辑回归及似然函数

首先假设：
$$
P(y=1|x;θ)=h_θ(x)
$$

$$
P(y=0|x;θ)=1-h_θ(x)
$$

$$
P(y|x;θ)=(h_θ(x))^y(1-h_θ(x))^{1-y}
$$

根据似然函数如下：
$$
L(θ)=P(Y|X;θ)=\prod_{i=1}^{m}{p(y^i|x^i;θ)}
$$
取对数，为对数似然函数：
$$
l(θ)=\log L(θ)=\sum_{i=1}^{m}{y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))}
$$

> 注：$h_θ(x^i)=g(θ^Tx^i)$

### 损失函数

* 由上述的对数似然函数出以下结论：在取某θ时，数似然函数取最大值，表面此时的参数为最优；


* 由此将似然函数取反，定义完整的损失函数如下：

$$
loss(h_\theta(x),y)=-\sum_{i=1}^{m}{y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))}
$$
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}{loss(h_\theta(x^{(i)},y^{(i)}))}
$$
$$
minJ(\theta)
$$
​	 通过梯度下降对J(θ)进行求解，结果如下：
$$
\frac{∂J(\theta)}{∂\theta_j}=\frac{1}{m}\sum_{i=1}^{m}{(g(θ^Tx^i)-y^i)x_j^i}
$$
​	更新参数
$$
\theta_j=\theta_j - \alpha\frac{∂J(\theta)}{∂\theta_j}
$$

### 正则化

同线性回归中梯度下降的正则化

### Softmax

#### 概念

* 逻辑回归只是处理**二分类**的问题，而Softmax回归则是逻辑回归的一般化，**适用于多分类(K类)的问题**；

* softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间

* softmax回归概率函数为 

$$
p(y=k|x;θ)=\frac{e^{θ_k^Tx}}{\sum_{j=1}^{K}{e^{θ_j^Tx}}},k=1,2,3,...,K
$$

> 其中，x表示其中一条数据，等价于$x^i$；$θ_{Kn}$为K类的参数矩阵，如下：
> $$
> x=\left[ \begin{matrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{matrix} \right]            θ_k^T=\left[ \begin{matrix} θ_{k1} \\ θ_{k2} \\ \vdots \\ θ_{kn} \end{matrix} \right]   θ_{Kn} = \left\{ \begin{matrix} θ_{11} & θ_{12} & \cdots & θ_{1n}\\ θ_{21} & θ_{22} & \cdots & θ_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ θ_{K1} & θ_{K2} & \cdots & θ_{Kn} \end{matrix} \right\}
> $$

* softmax对样本x进行预测：

$$
h_θ(x) = \left[ \begin{matrix} p(y=1|x;θ) \\ p(y=2|x;θ) \\ \vdots \\ p(y=K|x;θ) \end{matrix} \right]=\frac{1}{\sum_{j=1}^{K}{e^{θ_j^Tx}}}\left[ \begin{matrix} e^{θ_1^Tx} \\ e^{θ_2^Tx} \\ \vdots \\ e^{θ_K^Tx} \end{matrix} \right]
$$

> 将每个类别的预测值映射到[0,1]之间，取结果的最大值作为该类别的预测概率

* 损失函数：

$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}{}\sum_{j=1}^{K}{I(y^i=j)\ln \frac{e^{θ_j^Tx^i}}{\sum_{l=1}^{K}{e^{θ_l^Tx^i}}}},I(y^i=j)=\begin{cases} 1，y^i=j\\ 0， y^i\neq j\end{cases}
$$

* 梯度下降求解：

  对上述的损失函数$J(θ)$，对$θ_j$求偏导：

$$
\frac{∂J(\theta)}{∂\theta_j}=...=-I(y^i=j)(1-\frac{e^{θ_j^Tx^i}}{\sum_{l=1}^{K}{e^{θ_l^Tx^i}}})x^i
$$

​	   之后不停的进行梯度下降，不停更新参数：
$$
\theta_j=\theta_j - \alpha\frac{∂J(\theta)}{∂\theta_j}
$$

## 实际应用

### sklearn中API介绍

```python
sklearn.linear_model.LogisticRegression(penalty=‘l2’, C = 1.0)

# Logistic回归分类器
# coef_：回归系数
# 注：默认使用了l2正则化
```

案例——肿瘤良性、恶性预测

```python
# 导入必要的库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# 读取数据
# 构造列标签名字
column = ['Sample code number','Clump Thickness', 'Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion', 'Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']

# 读数据
data = pd.read_csv("./breast-cancer-wisconsin.data", names=column)
# print(data)

# 对缺失值进行处理
data = data.replace(to_replace="?", value=np.nan)

# 丢弃缺失值
data = data.dropna()

# 对数据进行分割
x_train, x_test, y_train, y_test = train_test_split(data[column[1:10]], data[column[10]], test_size=0.25)

# 进行标准化处理
std = StandardScaler()

x_train = std.fit_transform(x_train)
x_test = std.transform(x_test)

# 逻辑回归预测
lg = LogisticRegression(C=1.0)

lg.fit(x_train, y_train)
print(lg.coef_)
# [[1.18661055 0.37775507 0.77048383 0.72337353 0.40801567 1.31255209  0.72011434 0.52286549 0.69823535]]

# 模型评估
y_predict = lg.predict(x_test)

# 准确率
print("准确率为：", lg.score(x_test, y_test))
# 准确率为： 0.9649122807017544

from sklearn.metrics import classification_report
# 召回率
print("召回率为：", classification_report(y_test, y_predict, labels=[2, 4], target_names=["良性", "恶性"]))
# 召回率为：          precision  recall    f1-score    support
# 
#          良性       0.97      0.97      0.97       116
#          恶性       0.95      0.95      0.95        55
# 
# avg / total        0.96      0.96      0.96       171
```

* 其他参考代码，例如乳腺癌分类/鸢尾花数据分类，放置GitHub下，传送门：https://github.com/zhuChengChao/ML-LogisticsRegression
