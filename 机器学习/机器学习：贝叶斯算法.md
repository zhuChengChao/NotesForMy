# 机器学习：贝叶斯算法

> 贝叶斯算法作为机器学习的十大经典算法之一，在推动机器学习技术的发展的作用不言而喻；本文为个人在学习贝叶斯算法使所作的笔记。

## 相关公式

**先验概率P(A)**：在不考虑任何情况下，A事件发生的概率；

**条件概率P(B|A)**：A事件发生的情况下，B事件发生的概率；

**后验概率P(A|B)**：在B事件发生之后，对A事件发生的概率的重新评估；

**全概率**：若对A和A'构成样本空间的一个划分，那么事件B的概率为：A和A'的概率分别乘以B对这个两个事件的概率之和，如下：
$$
P(B)=P(A)*P(B|A)+P(A')*P(B|A')
$$

$$
P(B)=\sum_{i=1}^{n}{P(A_i)*P(B|A_i)}
$$

### 贝叶斯公式

**基于条件概率的贝叶斯定律公式**：（执果索因）
$$
P(B_i|A)=\frac{P(AB_i)}{P(A)}~~~~~~~~~~~~~~~~P(A)=\sum_{i=1}^{n}{P(B_i)*P(A|B_i)}
$$

$$
P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_{i=1}^{n}{P(B_i)*P(A|B_i)}}
$$

> 例子：
>
> 现将邮件分为两类：$A_1$代表垃圾邮件，$A_2$代表非垃圾邮件；根据经验$P(A_1)=0.7, P(A_2)=0.3$。
>
> 令B事件为邮件中包含"免费"这一关键词，有历史邮件可知，$P(B|A_1)=0.9, P(B|A_2)=0.01$。
>
> 问：若收到一封新邮件，包含了"免费"这一关键字，那么它是垃圾邮件的概率是多少？
> $$
> 需要求解的是：P(A_1|B),由条件概率公式可得：P(A_1|B)=\frac{P(A_1B)}{P(B)}
> $$
>
> $$
> 由贝叶斯公式得：P(A_1|B)=\frac{P(B|A_1)P(A_1)}{P(B|A_1)P(A_1)+P(B|A_2)P(A_2)}
> $$
>
> $$
> =\frac{0.9*0.7}{0.9*0.7+0.01*0.3} \approx 0.9953
> $$

## 朴素贝叶斯

### 朴素贝叶斯

Naive Bayes，是基于**特征之间的独立的**这一假设，应用贝叶斯定理的监督学习算法推导如下：

* 对于给定的样本X的特征向量为$(x_1,x_2,..,x_m)$;
* 该样本X的类别y的概率可以由贝叶斯公式得到，如下：

$$
P(y|x_1,x_2,..,x_m)=\frac{P(y)P(x_1,x_2,..,x_m|y)}{P(x_1,x_2,..,x_m)}=\frac{P(y)\prod_{i=1}^{m}{P(x_i|y)}}{P(x_1,x_2,..,x_m)}
$$

> $$
> 在特征之间是独立的前提下，有P(x_1,x_2,...,x_m)=P(x_1)P(x_2)...P(x_m),在加上条件概率后此公式也成立
> $$

* 在给定样本的情况下，$P(x_1,x_2,..,x_m)$为常数，因此有：

$$
P(y|x_1,x_2,..,x_m)⇒P(y)\prod_{i=1}^{m}{P(x_i|y)}
$$

* 从而，得出预测值如下：

$$
\hat{y}=argmax_yP(y)\prod_{i=1}^{m}{P(x_i|y)}
$$

> 上式结果中的P(y)和$P(x_i|y)$都可以用大数定理对样本进行统计而得出

#### 朴素贝叶斯算法流程 

* 设$x=(a_1,a_2,..,a_m)$为待分类项，其中a为x的一个特征属性；
* 设类别集合为$C=\{y_1,y_2,...,y_n\}$；
* 采用贝叶斯公式，分别计算$P(y_1|x),P(y_2|x),...,P(y_n|x)$的值；
* 如果$P(y_k|x)=max\{P(y_1|x),P(y_2|x),...,P(y_n|x)\}$，那么认为x为$y_k$类型。

![朴素贝叶斯流程](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742602.PNG)

> 其中对于$P(y_i), P(x|y_i)$的计算都可对样本进行统计而得出

### 高斯朴素贝叶斯

Gaussian Naive Bayes，是指：当特征属性，即X的取值为**连续值**时，**而且服从高斯分布**，那么在计算P(x|y)的时候，可以直接使用高斯分布的概率公式：
$$
g(x,η,σ)=\frac{1}{\sqrt{2π}σ}e^{-\frac{(x-η)^2}{2σ^2}}
$$

$$
P(x_k|y_k)=g(x_k,η_{y_k},σ_{y_k})
$$

由上式公式得，只需要计算出类别k的均值和标准差，代入计算概率即可。

### 伯努利朴素贝叶斯

Bernoulli Naive Bayes，是指：当特征属性为连续值时，且分布服从伯努利分布，那么在计算P(x|y)的时候，可以直接使用伯努利分布的概率公式。

> -.-?
>
> 此处特征属性为连续值解释如下：当特征属性有缺失值时，为1表示此事的x有值，为0表示此时特征属性x是空缺值

$$
P(x_k|y)=P(1|y)x_k+(1-P(1|y))(1-x_k)
$$

> 伯努利分布为一种离散分布，只有1/0两者结果，若1出现的概率为p，则0出现的概率相应就为q=1-p；
>
> 其中均值为E(x)=p，方差为Var(X)=p(1-p)

### 多项式朴素贝叶斯

> **多项式分布介绍：**
>
> 多项式分布是二项式分布的推广，
> $$
> 在多项式分布中，每次的实验有k个结果A_1,A_2,...,A_k
> $$
>
> $$
> 分别将他们出现次数记为随机变量x_1,x_2,...,x_k,它们的概率分布分别为p_1,p_2,...,p_k
> $$
>
> $$
> 那么在n次采样的结果中，A_1出现n_1次，A_2出现n_2次,...,A_k出现n_k次这种事件出现的概率P有如下公式
> $$
>
> $$
> P(x_1=n_1,x_2=n_2,...,x_k=n_k)=\begin{cases} n!\prod_{i=1}^{k}{\frac{p_i^{n_i}}{n_i!}}，\sum_{i=1}^{k}{n_i=n}\\ 0， otherwise\end{cases}
> $$
>
> > 最简单的例子就是多次抛筛子，统计各个面被掷中的次数

Multinomial Naive Bayes，是指：当特征属性服从多项式分布，从而，对于每个类别y，参数为
$$
θ_y=(θ_{y1},θ_{y2},...,θ_{yn}),其中n为特征属性数目
$$
其中$θ_{yi}$表示特征i在类别y中出现的概率，即为$P(x_i|y)$，
$$
θ_{yi}=\frac{N_{yi}+α}{N_{y}+α*n}，α为平滑系数
$$

$$
训练集T上特征i在类别y中出现的次数：N_{yi}=\sum_{x∈T}{x_i}
$$

$$
类别y的所有特征的总数：N_y=\sum_{i=1}^{n}{N_{yi}}
$$

> 举个例子，有如下的样本：
>
> |      | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |
> | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
> | x_1  | 1    | 1    | 1    | 1    | 1    | 2    | 2    | 2    | 2    | 2    | 3    | 3    | 3    | 3    | 3    |
> | x_2  | S    | M    | M    | S    | S    | S    | M    | M    | L    | L    | L    | M    | M    | L    | L    |
> | Y    | -1   | -1   | 1    | 1    | -1   | -1   | -1   | 1    | 1    | 1    | 1    | 1    | 1    | 1    | -1   |
>
> $$
> 此处先令α=1
> $$
>
> 计算先验概率：
> $$
> P(Y=1)=\frac{9+1}{15+1*2}=\frac{10}{17}~~~~~~~~~~~~~P(Y=-1)=\frac{6+1}{15+1*2}=\frac{7}{17}
> $$
> 计算条件概率：
> $$
> P(X_1=1|Y=1)=\frac{2+1}{9+1*2}~~~P(X_1=2|Y=1)=\frac{3+1}{9+1*2}~~~P(X_1=3|Y=1)=\frac{4+1}{9+1*2}
> $$
>
> $$
> P(X_2=S|Y=1)=\frac{1+1}{9+1*2}~~~P(X_2=M|Y=1)=\frac{4+1}{9+1*2}~~~P(X_2=L|Y=1)=\frac{4+1}{9+1*2}
> $$
>
> $$
> P(X_1=1|Y=-1)=\frac{3+1}{6+1*2}~~~P(X_1=2|Y=-1)=\frac{2+1}{6+1*2}~~~P(X_1=3|Y=-1)=\frac{1+1}{6+1*2}
> $$
>
> $$
> P(X_2=S|Y=-1)=\frac{3+1}{6+1*2}~~~P(X_2=M|Y=-1)=\frac{2+1}{6+1*2}~~~P(X_2=L|Y=-1)=\frac{1+1}{6+1*2}
> $$
>
> > ？？？如按照公式来，分母应该为11和8，而在很多博客上的截图上，分母却为12和9，暂时不知道是哪里的问题
>
> 若给定一个样本x=(2,s)，则可通过上述计算的先验概率和条件概率求得：
> $$
> P(Y=1)P(X_1=2|Y=1)P(X_2=S|Y=1)~~~~~~~P(Y=-1)P(X_1=2|Y=-1)P(X_2=S|Y=-1)
> $$
> 根据结果大小来确定分类值

### 分布方式选择

* 如果x是连续的一般选择高斯朴素贝叶斯；
* 如果x是离散的一般选择多项式朴素贝叶斯；
* 如果x即有连续又有离散，一般选择高斯朴素贝叶斯。

## 贝叶斯网络

### 概念

* 对系统中的**随机变量**，根据其条件是否独立，绘制在一个**有向图**中，就形成了贝叶斯网络(Bayesian Network)；

* 贝叶斯网络为**有向无环图模型**，是一种概率图模型，根据概率图的拓扑结构，考察一组随机变量$\{X_1,X_2,...,X_n\}$及N组**条件概率分布**的性质。

* 当多个特征属性之间存在某种相关关系的时候，使用朴素贝叶斯算法就没办法解决了，此时采用贝叶斯网络就是很好的解决该类应用场景的算法。

* 一般而言，贝叶斯网络的有向无环图中的**节点表示随机变量**(可以是可观察到的变量，或隐变量，未知参数等),连接两个节点之间的**箭头代表两个随机变量之间的因果关系(即这两个随机变量之间非条件独立)**，如果两个节点间以一个单箭头连接在一起，表示其中一个节点是"因"，另一个是"果"，从而两节点之间就会产生一个条件概率值。

* 贝叶斯网络是模拟人的认知思维推理模式的，用一组条件概率以及有向无环图对不确定性因果推理关系建模 

### 图解贝叶斯网络

#### 最简单的一个贝叶斯网络

![贝叶斯网络1](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742593.PNG)
$$
由上图，得出abc同时发生的概率：P(a,b,c)=P(a)P(b|a)P(c|a,b)
$$

#### 全连接贝叶斯网络

![贝叶斯网络2](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742155.PNG)
$$
P(A,B,C,D,E)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)P(E|A,B,C,D)
$$

#### “正常“的贝叶斯网络

![贝叶斯网络3](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742846.PNG)

由上图，可见：

* $x_1,x_2,x_3$独立；
* $x_6,x_7$在给定条件下也独立；

联合概率分布为：
$$
P(x_1,x_2,x_3,x_4,x_5,x_6,x_7)=P(x_1)P(x_2)P(x_3)P(x_4|x_1,x_2,x_3)P(x_5|x_1,x_2,x_3)P(x_6|x_4)P(x_7|x_4,x_5)
$$

> 举个列子：
>
> 有一天早晨，Bruce离开他的房子的时候发现他家花园中的草地是湿的，有两种可能，第一：昨天晚上下雨了，第二：他昨天晚上忘记关掉花园中的喷水器，接下来，他观察他的邻居Joe，发现他家花园中的草地也是湿的，因此，他推断，他家的草地湿了是因为昨天晚上下雨的缘故。
>
> 图表示如下：
>
> ![判断是否下雨](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742910.PNG)
>
> | 事件               | 概率1                 | 概率2               |
> | ------------------ | --------------------- | ------------------- |
> | P(R)为是否下雨     | 0没有下雨：P(R=0)=0.8 | 1下雨：P(R=1)=0.2   |
> | P(S)为是否关喷水器 | 0关了：P(R=0)=0.9     | 1没有关：P(R=1)=0.1 |
>
> | R是否下雨               | 0：没有下雨 | 1：下雨 |
> | ----------------------- | ----------- | ------- |
> | Joe的草地是干的：P(J=0) | 0.8         | 0       |
> | Joe的草地是湿的：P(J=1) | 0.2         | 1       |
>
> | R是否下雨 | 是否关喷水器 | Bruce草地干:P(B=0) | Bruce草地湿:P(B=1) |
> | --------- | ------------ | ------------------ | ------------------ |
> | 0         | 0            | 1                  | 0                  |
> | 0         | 1            | 0.1                | 0.9                |
> | 1         | 0            | 0                  | 1                  |
> | 1         | 1            | 0                  | 1                  |
>
> 因此，可得出Bruce家草地湿了是因为下雨的缘故为：
> $$
> P(S=1|B=1,J=1)=\frac{P(S=1,B=1,J=1)}{P(B=1,J=1)}
> $$
>
> $$
> =\frac{\sum{_R}{P(S=1,B=1,J=1,R)}}{\sum_{R,S}{P(B=1,J=1,R,S)}}
> $$
>
> $$
> =\frac{\sum{_R}{P(R)P(S=1)P(J=1|R)P(B=1|R,S=1)}}{\sum_{R,S}{P(R)P(S)P(J=1|R)P(B=1|R,S)}}=\frac{0.0344}{0.2144}=0.1604
> $$
>
> 同理：
> $$
> P(R=1|B=1,J=1)=...=\frac{0.2}{0.2144}=0.9328
> $$
> 因此，判断应该是下雨导致的草地变湿

### 贝叶斯网络判定条件独立

#### tail-to-tail

下图为**C给定的条件下**，a和b被阻断(blocked)的情况，此时a和b为独立

![条件独立1](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742164.PNG)
$$
P(a,b,c)=P(c)P(a|c)P(b|c)
$$

$$
⇒\frac{P(a,b,c)}{P(c)}=P(a|c)P(b|c)
$$

$$
∵P(a,b|c)=\frac{P(a,b,c)}{P(c)}
$$

$$
∴P(a,b|c)=P(a|c)P(b|c)
$$

#### head-to-tail

下图为**C给定的条件下**，a和b被阻断(blocked)的情况，此时a和b为独立

![条件独立2](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742613.PNG)
$$
P(a,b,c)=P(a)P(c|a)P(b|c)
$$

$$
P(a,b|c)=\frac{P(a,b,c)}{P(c)}=\frac{P(a)P(c|a)P(b|c)}{P(c)}
$$

$$
=\frac{P(a,c)P(b|c)}{P(c)}=\frac{P(c|a)P(c)P(b|c)}{P(c)}=P(c|a)P(b|c)
$$

#### head-to-head

下图为**C未知的情况下**，a和b被阻断(blocked)的情况，此时a和b为独立

![条件独立3](https://xycnotes.oss-cn-hangzhou.aliyuncs.com/img/202206251742348.PNG)
$$
P(a,b,c)=P(a)P(b)P(c|a,b)
$$

$$
\sum_{c}{P(a,b,c)}=\sum_{c}{P(a)P(b)P(c|a,b)}
$$

$$
左边：遍历所有的C相加⇒\sum_{c}{P(a,b,c)}=P(a,b)
$$

$$
右边：\sum_{c}{P(a)P(b)P(c|a,b)}=P(a)P(b)\sum_{c}{P(c|a,b)}=P(a)P(b)
$$

$$
∴P(a,b)=P(a)P(b)
$$

## 实际应用

* 01_贝叶斯算法案例一：鸢尾花数据分类
* 02_贝叶斯算法案例二：新闻数据分类

相关代码提交于GitHub，传送门：https://github.com/zhuChengChao/ML-Bayes-EM

