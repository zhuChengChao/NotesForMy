# æœºå™¨å­¦ä¹ ï¼šçº¿æ€§å›å½’

> çº¿æ€§å›å½’æ˜¯åˆ©ç”¨å›å½’æ–¹ç¨‹å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å…³ç³»è¿›è¡Œå»ºæ¨¡çš„ä¸€ç§åˆ†ææ–¹å¼ã€‚
>
> ä¸€èˆ¬å°†å…¶ä½œä¸ºå¼€å§‹å­¦ä¹ æœºå™¨å­¦ä¹ çš„èµ·ç‚¹...é‚£å°±ä»¥æ­¤ä½œä¸ºé¦–ç¯‡åšå®¢ :smile:

## ç†è®ºåˆ†æ

### å®šä¹‰

* å›å½’ç®—æ³•æ˜¯ä¸€ç§**æœ‰ç›‘ç£ç®—æ³•**

* **çº¿æ€§å›å½’**é€šè¿‡ä¸€ä¸ªæˆ–è€…å¤šä¸ª**è‡ªå˜é‡(X)**ä¸**å› å˜é‡(Y)**ä¹‹é—´è¿›è¡Œå»ºæ¨¡çš„å›å½’åˆ†æï¼Œå³åœ¨ç®—æ³•çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæœºå™¨è¯•å›¾å¯»æ‰¾ä¸€ä¸ª**å‡½æ•°h(ypothesis)**:ä½¿å¾—è‡ªå˜é‡Xåœ¨hçš„ä½œç”¨ä¸‹äº§ç”Ÿçš„ç»“æœå°½å¯èƒ½çš„å› å˜é‡Yç›¸æ¥è¿‘ï¼›
* å›å½’ç®—æ³•å¯åˆ†ä¸ºï¼š
  * ä¸€å…ƒçº¿æ€§å›å½’ï¼šæ¶‰åŠåˆ°çš„å˜é‡åªæœ‰ä¸€ä¸ª
  * å¤šå…ƒçº¿æ€§å›å½’ï¼šæ¶‰åŠåˆ°çš„å˜é‡ä¸¤ä¸ªæˆ–ä¸¤ä¸ªä»¥ä¸Š

### é€šç”¨å…¬å¼

$$
â„(Î¸)= Î¸_0+Î¸_1 ğ‘¥_1+Î¸_2ğ‘¥_2+â€¦+Î¸_nğ‘¥_n= Î¸^ğ‘‡ ğ‘¥
$$

> å…¶ä¸­xå’ŒÎ¸åˆ†åˆ«ä¸ºï¼š
> $$
> x=\left[\begin{matrix} x_{0} \\ x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{matrix} \right] Î¸=\left[\begin{matrix} Î¸_{0} \\ Î¸_{1} \\ Î¸_{2} \\ \vdots \\ Î¸_{n} \end{matrix} \right]
> $$
>
> æ­¤å¤„çš„xè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œç­‰ä»·äº$x^{i}$è¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬ï¼›å…¶ä¸­**n**ä¸ºç‰¹å¾çš„æ•°é‡

### æŸå¤±å‡½æ•°

å°†æ¨¡å‹ä¸æ•°æ®ç‚¹ä¹‹é—´çš„è·ç¦»å·®çš„å¹³æ–¹å’Œåšä¸ºè¡¡é‡åŒ¹é…å¥½åçš„æ ‡å‡†ï¼Œè¯¯å·®è¶Šå°ï¼ŒåŒ¹é…ç¨‹åº¦è¶Šå¤§ã€‚
$$
J(Î¸) = \frac{1}{2m}\sum_{i=0}^{m}{(h_Î¸(x^{(i)})-y^{(i)})^2}
$$
è¯¯å·®è¶Šå°ï¼ŒåŒ¹é…ç¨‹åº¦è¶Šå¤§ï¼Œå› æ­¤ï¼Œéœ€è¦åšåˆ°çš„æ˜¯ä¸æ–­çš„è°ƒæ•´å‚æ•°Î¸ï¼Œä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚

> æ­¤æ–¹æ³•å³ä¸ºæœ€å°äºŒä¹˜æ³•ï¼›**m**ï¼šä¸ºæ ·æœ¬çš„æ•°é‡

### å‚æ•°æ±‚è§£

#### æ±‚è§£æ–¹å¼ä¸€ï¼šæ­£è§„æ–¹ç¨‹

$$
Î¸ = (X^TX)^{-1}X^TY
$$

> å…¶ä¸­Xä¸ºç‰¹å¾æ–¹ç¨‹çŸ©é˜µï¼Œyä¸ºç›®æ ‡å€¼çŸ©é˜µï¼š
> $$
> X=\left[ \begin{matrix} (x^{0})^{T} \\ (x^{1})^{T} \\ (x^{2})^{T} \\ \vdots \\ (x^{m})^{T} \end{matrix} \right]_{m*n} Y = \left[ \begin{matrix} y_{0} \\ y_{1} \\ y_{2} \\ \vdots \\ y_{m} \end{matrix} \right]_{m*1}
> $$
> 

> æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼š
>
> $$
> J(Î¸) = \frac{1}{2m}\sum_{i=0}^{m}{ï¼ˆh_Î¸(x^{(i)})-y^{(i)})^2}=\frac{1}{2m}(XÎ¸-Y)^{T}(XÎ¸-Y)
> $$
>
> $$
> éœ€è¦å–åˆ°Î¸å€¼ï¼Œä½¿å¾—J(Î¸)æœ€å°ï¼Œå› æ­¤å¯¹J(Î¸)æ±‚è§£æ¢¯åº¦
> $$
>
> $$
> \nabla_{Î¸}J(Î¸)=\nabla_{Î¸}(\frac{1}{2}(XÎ¸-Y)^{T}(XÎ¸-Y))
> $$
>
> $$
> =\nabla_{Î¸}(\frac{1}{2}(Î¸^{T}X^{T}-Y^{T})(XÎ¸-Y))
> $$
>
> $$
> =\nabla_{Î¸}(\frac{1}{2}(Î¸^{T}X^{T}XÎ¸-Y^{T}XÎ¸-Î¸^{T}X^{T}Y+Y^{T}Y))
> $$
>
> $$
> =\frac{1}{2}(2X^{T}XÎ¸-X^{T}Y-(Y^{T}X)^{T})
> $$
>
> $$
> =X^{T}XÎ¸-X^{T}Y
> $$
>
> $$
> å½“æ¢¯åº¦å€¼ä¸º0æ—¶ï¼Œå–åˆ°æœ€ä¼˜çš„Î¸å€¼ï¼Œå³
> $$
>
> $$
> Î¸ = (X^{T}X)^{-1}X^{T}Y
> $$

**ç¼ºç‚¹ï¼š**

* å½“ç‰¹å¾è¿‡äºå¤æ‚ï¼Œæ±‚è§£é€Ÿåº¦å¤ªæ…¢
* å¯¹äºå¤æ‚çš„ç®—æ³•ï¼Œä¸èƒ½ä½¿ç”¨æ­£è§„æ–¹ç¨‹æ±‚è§£(é€»è¾‘å›å½’ç­‰)

#### æ±‚è§£æ–¹å¼äºŒï¼šæ¢¯åº¦ä¸‹é™

æŸå¤±å‡½æ•°ä¸ºï¼š

$$
J(Î¸) = \frac{1}{2m}\sum_{i=0}^{m}{ï¼ˆh_Î¸(x^{(i)})-y^{(i)})^2}
$$

å¯¹ä¸Šè¿°çš„Î¸æ±‚åå¯¼
$$
\frac{âˆ‚J(Î¸)}{âˆ‚Î¸_j}=\frac{1}{2m}\frac{âˆ‚\sum_{i=0}^{m}{ï¼ˆh_Î¸(x^{(i)})-y^{(i)})^2}}{âˆ‚Î¸_j}
$$
å‚æ•°æ›´æ–°ï¼š
$$
Î¸_j := Î¸_j - \alpha\frac{1}{m}[\sum_{i=0}^{m}{ï¼ˆh_Î¸(x^{(i)})-y^{(i)})}x^{i}_{j}]
$$

$$
j=0,1,2,...,n
$$

ä¸€ç›´è¿­ä»£ï¼Œç›´è‡³æ”¶æ•›

> Î±ä¸ºå­¦ä¹ é€Ÿç‡ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š

### æ­£åˆ™åŒ–

#### è¿‡æ‹Ÿåˆè§£å†³

* å‡å°‘ç‰¹å¾é‡ï¼›
* æ­£åˆ™åŒ–ï¼šä¹Ÿå°±æ˜¯çš„Î¸å€¼åœ¨æ ·æœ¬ç©ºé—´ä¸­ä¸èƒ½è¿‡å¤§/è¿‡å°ï¼Œåœ¨ç›®æ ‡å‡½æ•°ä¹‹ä¸Šå¢åŠ ä¸€ä¸ªå¹³æ–¹å’ŒæŸå¤± 

#### æ¢¯åº¦ä¸‹é™æ­£åˆ™åŒ–

* `L2-norm`ï¼šå²­å›å½’(Ridge)

ä¿®æ”¹ä»£ä»·å‡½æ•°ï¼Œç”¨ä»¥ç¼©å°æ‰€æœ‰å‚æ•°ï¼š
$$
J(Î¸)=\frac{1}{2m}[\sum_{i=0}^{m}{h_Î¸(x^{(i)})-y^{(i)})^2} + \lambda\sum_{j=0}^{n}{Î¸^{2}_{j}}]
$$
å†è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼š
$$
Î¸_j := Î¸_j - \alpha\frac{âˆ‚J(Î¸)}{âˆ‚Î¸_j}=Î¸_j - \alpha[\frac{1}{m}\sum_{i=0}^{m}(h_Î¸(x^{(i)}-y^{(i)})x^{(i)}_j+\frac{\lambda}{m}Î¸_j]
$$
å¯å¾—
$$
Î¸_j=Î¸_j(1-\alpha\frac{\lambda}{m}) -\alpha[\frac{1}{m}\sum_{i=0}^{m}(h_Î¸(x^{(i)}-y^{(i)})x^{(i)}_j
$$

* `L1-norm`ï¼šLASSOå›å½’

$$
J(Î¸)=\frac{1}{2m}[\sum_{i=0}^{m}{h_Î¸(x^{(i)})-y^{(i)})^2} + \lambda\sum_{i=0}^{m}{|Î¸_{j}|}]
$$

* `L1-norm`+`Lw2-norm`ï¼šå¼¹æ€§ç½‘ç»œ(Elastic Net )

$$
J(Î¸)=\frac{1}{2m}[\sum_{i=0}^{m}{h_Î¸(x^{(i)})-y^{(i)})^2}] + \lambda[p\sum_{j=0}^{n}{|Î¸_{j}|}+(1-p)\sum_{j=0}^{n}{|Î¸_{j}^2|}]
$$

### æœ€å¤§ä¼¼ç„¶ä¼°è®¡çº¿æ€§å›å½’

å¯¹äºæ¯ä¸€ç»„Î¸å‚æ•°ä½œç”¨ä¸‹ï¼Œæœ‰å¦‚ä¸‹å…¬å¼ï¼š
$$
y^i = Î¸^Tx^i+Îµ^i
$$
ç”±äº**ä¸­å¿ƒæé™å®šç†**ï¼Œå…¶è¯¯å·®Îµæ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæœä»å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸ºæŸå®šå€¼$Ïƒ^2$çš„é«˜æ–¯åˆ†å¸ƒï¼Œå¦‚ä¸‹ï¼š
$$
p(Îµ^i)=\frac{1}{Ïƒ\sqrt{2Ï€}}e^{-\frac{(Îµ^i)^2}{2Ïƒ^2}}
$$
å¯¹äºä¸€ä¸ªç‰¹å®šçš„Î¸ï¼Œç»™å®šçš„xåœ¨Î¸çš„ä½œç”¨ä¸‹ï¼Œäº§ç”Ÿyçš„å¯èƒ½æ€§æ¦‚ç‡å¦‚ä¸‹ï¼š
$$
p(y^i|x^i;Î¸)=\frac{1}{Ïƒ\sqrt{2Ï€}}e^{-\frac{(y^i-Î¸^Tx^i)^2}{2Ïƒ^2}}
$$
å¯¹äºæ‰€æœ‰çš„æ ·æœ¬ï¼Œæ ¹æ®ä¼¼ç„¶å‡½æ•°å¦‚ä¸‹ï¼š
$$
L(Î¸)=\prod_{i=1}^{m}{p(y^i|x^i,Î¸)}=\prod_{i=1}^{m}{\frac{1}{Ïƒ\sqrt{2Ï€}}e^{-\frac{(y^i-Î¸^Tx^i)^2}{2Ïƒ^2}}}
$$
è€Œç›®å‰xä¸yå€¼æ˜¯ç¡®å®šçš„ï¼Œå› æ­¤è¦å¯»æ‰¾ä¸€ä¸ªæœ€å¤§çš„Î¸å€¼ï¼Œä½¿å¾—L(Î¸)çš„å€¼æœ€å¤§ï¼›è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼šä¸¤è¾¹å–å¯¹æ•°ï¼Œä¹‹åç»è¿‡åŒ–ç®€å¯å¾—å¦‚ä¸‹å…¬å¼ï¼š
$$
L(Î¸)=m\ln\frac{1}{Ïƒ\sqrt{2Ï€}}-\frac{1}{Ïƒ^2}\frac{1}{2}\sum_{i=0}^{m}{(y^i-Î¸^Tx^i)^2}
$$
ä¸Šå¼ä¸­åªæœ‰ä¸€é¡¹å¸¦æœ‰Î¸å€¼ï¼Œå…¶ä»–éƒ½ä¸ºå¸¸æ•°ï¼Œå› æ­¤åªéœ€æœ€å¤§åŒ–è¯¥é¡¹ï¼Œå°†è¯¥é¡¹å–åå³å¦‚ä¸‹æ‰€ç¤ºï¼š
$$
min_Î¸\frac{1}{2}\sum_{i=0}^{m}{(y^i-Î¸^Tx^i)^2}
$$
å¯è§ç»“æœä¸æœ€å°äºŒä¹˜æ³•çš„ç»“æœæ˜¯ä¸€è‡´çš„ï¼Œå¯ä»¥å°†æœ€å°äºŒä¹˜çœ‹æˆæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚

> ä»¥ä¸‹ä¸¤ç¯‡åšæ–‡ä»‹ç»è¿™ä¸ªæ¦‚å¿µä»‹ç»çš„è¿˜è¡Œï¼š
>
> https://www.jianshu.com/p/fbd736a61927
>
> https://blog.csdn.net/ppn029012/article/details/8908104

## å®é™…åº”ç”¨

é€šè¿‡sklearnä¸­çš„APIï¼Œå¯ä»¥æ–¹ä¾¿çš„å®ç°çº¿æ€§å›å½’

* çº¿æ€§å›å½’é¢„æµ‹æ³¢å£«é¡¿æˆ¿ä»·â€”â€”æ­£è§„æ–¹ç¨‹

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error

def liner_regression_model():
    """
    çº¿æ€§å›å½’ç›´æ¥é¢„æµ‹æ³¢å£«é¡¿æˆ¿ä»·
    """
    # è·å–æ•°æ®
    lb = load_boston()

    # åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œç›®æ ‡é›†
    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=0.25)
    # print(x_train, y_train)
    print("test dataset's price:", y_test)

    # ç‰¹å¾å€¼å’Œç›®æ ‡å€¼æ˜¯éƒ½å¿…é¡»è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†, å®ä¾‹åŒ–ä¸¤ä¸ªæ ‡å‡†åŒ–API
    std_x = StandardScaler()
    x_train = std_x.fit_transform(x_train)
    x_test = std_x.transform(x_test)

    std_y = StandardScaler()
    y_train = std_y.fit_transform(y_train.reshape(-1,1))
    y_test = std_y.fit_transform(y_test.reshape(-1,1))

    # é¢„æµ‹æˆ¿ä»·ä»·æ ¼(æ­£è§„æ–¹ç¨‹) æ™®é€šæœ€å°äºŒä¹˜çº¿æ€§å›å½’
    # æ‹Ÿåˆæ¨¡å‹ï¼Œå¾—å‡ºå‚æ•°
    lr = LinearRegression()
    lr.fit(x_train, y_train)
    print(lr.coef_)  
    # è¡¨ç¤ºå›å½’ç³»æ•°w=(w1,w2....)
   	# é¢„æµ‹æµ‹è¯•æœºæˆ¿å­ä»·æ ¼
    y_lr_predict = std_y.inverse_transform(lr.predict(x_test))
    # print("lr test dataset's perdict:", y_lr_predict)
    print("lr mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))

```

* æ¨¡å‹ä¿å­˜API

```python
 from sklearn.externals import joblib
    
 # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
 # joblib.dump(lr, "./linearRegressionModel.pkl")
 # ç”¨ä¿å­˜å¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
 lr = joblib.load("./LinearRegressionModel.pkl")
 # é¢„æµ‹æµ‹è¯•æœºæˆ¿å­ä»·æ ¼
 y_lr_predict = std_y.inverse_transform(lr.predict(x_test))
 # print("lr test dataset's perdict:", y_lr_predict)
```

* å›å½’è¯„ä¼°APIâ€”â€”å‡æ–¹è¯¯å·®

```python
mean_squared_error(y_true,Â y_pred)
# å‡æ–¹è¯¯å·®å›å½’æŸå¤±
# y_true:çœŸå®å€¼
# y_pred:é¢„æµ‹å€¼
# return:æµ®ç‚¹æ•°ç»“æœ
```

```python
from sklearn.metrics import mean_squared_error

# æ¥ä¸Šè¿°ä»£ç 
print("lr mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))
```

* é‡‡ç”¨æ¢¯åº¦ä¸‹é™è¿›è¡Œé¢„æµ‹

```python
from sklearn.linear_model import SGDRegressor

 # é¢„æµ‹æˆ¿ä»·ä»·æ ¼ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
 # æ‹Ÿåˆæ¨¡å‹ï¼Œå¾—å‡ºå‚æ•°
 sgd = SGDRegressor()
 sgd.fit(x_train, y_train)
 y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))
 # print("test dataset's perdict:", y_sgd_predict)
 print("sgd mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))
```

* å²­å›å½’è¿›è¡Œé¢„æµ‹â€”â€”L2æ­£åˆ™åŒ–

```python
sklearn.linear_model.Ridge(alpha=1.0)
# å…·æœ‰l2æ­£åˆ™åŒ–çš„çº¿æ€§æœ€å°äºŒä¹˜æ³•

# alpha:æ­£åˆ™åŒ–åŠ›åº¦
# coef_:å›å½’ç³»æ•°
```

```python
from sklearn.linear_model import Ridge

rg = Ridge()
rg.fit(x_train, y_train)
y_rg_predict = std_y.inverse_transform(rg.predict(x_test))
# print("test dataset's perdict:", y_rg_predict)
print("rg mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_rg_predict))
```

* Lassoå›å½’â€”â€”L1æ­£åˆ™åŒ–
* ElasticNetCVå¼¹æ€§ç½‘ç»œ

> ä»æŸäº›æ•™ç¨‹è§†é¢‘ä¸­æ‰¾åˆ°äº†å¾ˆå¤šå€¼å¾—å‚è€ƒçš„ä»£ç ï¼Œéƒ½æ”¾åˆ°äº†æˆ‘çš„GitHubç½‘å€ä¸‹ï¼Œä¾‹å¦‚å¸¦L1æ­£åˆ™åŒ–çš„Lassoå›å½’ï¼ŒåŒæ—¶ä½¿ç”¨L1å’ŒL2æ­£åˆ™åŒ–çš„å¼¹æ€§ç½‘ç»œç®—æ³•...
>
> ä¼ é€é—¨ï¼šhttps://github.com/zhuChengChao/ML-LinearRegrssion

