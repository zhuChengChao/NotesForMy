# 机器学习：线性回归

> 线性回归是利用回归方程对一个或多个自变量和因变量之间关系进行建模的一种分析方式。
>
> 一般将其作为开始学习机器学习的起点...那就以此作为首篇博客 :smile:

## 理论分析

### 定义

* 回归算法是一种**有监督算法**

* **线性回归**通过一个或者多个**自变量(X)**与**因变量(Y)**之间进行建模的回归分析，即在算法的学习过程中，机器试图寻找一个**函数h(ypothesis)**:使得自变量X在h的作用下产生的结果尽可能的因变量Y相接近；
* 回归算法可分为：
  * 一元线性回归：涉及到的变量只有一个
  * 多元线性回归：涉及到的变量两个或两个以上

### 通用公式

$$
ℎ(θ)= θ_0+θ_1 𝑥_1+θ_2𝑥_2+…+θ_n𝑥_n= θ^𝑇 𝑥
$$

> 其中x和θ分别为：
> $$
> x=\left[\begin{matrix} x_{0} \\ x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{matrix} \right] θ=\left[\begin{matrix} θ_{0} \\ θ_{1} \\ θ_{2} \\ \vdots \\ θ_{n} \end{matrix} \right]
> $$
>
> 此处的x表示一个样本，等价于$x^{i}$表示第i个样本；其中**n**为特征的数量

### 损失函数

将模型与数据点之间的距离差的平方和做为衡量匹配好坏的标准，误差越小，匹配程度越大。
$$
J(θ) = \frac{1}{2m}\sum_{i=0}^{m}{(h_θ(x^{(i)})-y^{(i)})^2}
$$
误差越小，匹配程度越大，因此，需要做到的是不断的调整参数θ，使损失函数最小化。

> 此方法即为最小二乘法；**m**：为样本的数量

### 参数求解

#### 求解方式一：正规方程

$$
θ = (X^TX)^{-1}X^TY
$$

> 其中X为特征方程矩阵，y为目标值矩阵：
> $$
> X=\left[ \begin{matrix} (x^{0})^{T} \\ (x^{1})^{T} \\ (x^{2})^{T} \\ \vdots \\ (x^{m})^{T} \end{matrix} \right]_{m*n} Y = \left[ \begin{matrix} y_{0} \\ y_{1} \\ y_{2} \\ \vdots \\ y_{m} \end{matrix} \right]_{m*1}
> $$
> 

> 推导过程如下：
>
> $$
> J(θ) = \frac{1}{2m}\sum_{i=0}^{m}{（h_θ(x^{(i)})-y^{(i)})^2}=\frac{1}{2m}(Xθ-Y)^{T}(Xθ-Y)
> $$
>
> $$
> 需要取到θ值，使得J(θ)最小，因此对J(θ)求解梯度
> $$
>
> $$
> \nabla_{θ}J(θ)=\nabla_{θ}(\frac{1}{2}(Xθ-Y)^{T}(Xθ-Y))
> $$
>
> $$
> =\nabla_{θ}(\frac{1}{2}(θ^{T}X^{T}-Y^{T})(Xθ-Y))
> $$
>
> $$
> =\nabla_{θ}(\frac{1}{2}(θ^{T}X^{T}Xθ-Y^{T}Xθ-θ^{T}X^{T}Y+Y^{T}Y))
> $$
>
> $$
> =\frac{1}{2}(2X^{T}Xθ-X^{T}Y-(Y^{T}X)^{T})
> $$
>
> $$
> =X^{T}Xθ-X^{T}Y
> $$
>
> $$
> 当梯度值为0时，取到最优的θ值，即
> $$
>
> $$
> θ = (X^{T}X)^{-1}X^{T}Y
> $$

**缺点：**

* 当特征过于复杂，求解速度太慢
* 对于复杂的算法，不能使用正规方程求解(逻辑回归等)

#### 求解方式二：梯度下降

损失函数为：

$$
J(θ) = \frac{1}{2m}\sum_{i=0}^{m}{（h_θ(x^{(i)})-y^{(i)})^2}
$$

对上述的θ求偏导
$$
\frac{∂J(θ)}{∂θ_j}=\frac{1}{2m}\frac{∂\sum_{i=0}^{m}{（h_θ(x^{(i)})-y^{(i)})^2}}{∂θ_j}
$$
参数更新：
$$
θ_j := θ_j - \alpha\frac{1}{m}[\sum_{i=0}^{m}{（h_θ(x^{(i)})-y^{(i)})}x^{i}_{j}]
$$

$$
j=0,1,2,...,n
$$

一直迭代，直至收敛

> α为学习速率，需要手动指定

### 正则化

#### 过拟合解决

* 减少特征量；
* 正则化：也就是的θ值在样本空间中不能过大/过小，在目标函数之上增加一个平方和损失 

#### 梯度下降正则化

* `L2-norm`：岭回归(Ridge)

修改代价函数，用以缩小所有参数：
$$
J(θ)=\frac{1}{2m}[\sum_{i=0}^{m}{h_θ(x^{(i)})-y^{(i)})^2} + \lambda\sum_{j=0}^{n}{θ^{2}_{j}}]
$$
再进行梯度下降：
$$
θ_j := θ_j - \alpha\frac{∂J(θ)}{∂θ_j}=θ_j - \alpha[\frac{1}{m}\sum_{i=0}^{m}(h_θ(x^{(i)}-y^{(i)})x^{(i)}_j+\frac{\lambda}{m}θ_j]
$$
可得
$$
θ_j=θ_j(1-\alpha\frac{\lambda}{m}) -\alpha[\frac{1}{m}\sum_{i=0}^{m}(h_θ(x^{(i)}-y^{(i)})x^{(i)}_j
$$

* `L1-norm`：LASSO回归

$$
J(θ)=\frac{1}{2m}[\sum_{i=0}^{m}{h_θ(x^{(i)})-y^{(i)})^2} + \lambda\sum_{i=0}^{m}{|θ_{j}|}]
$$

* `L1-norm`+`Lw2-norm`：弹性网络(Elastic Net )

$$
J(θ)=\frac{1}{2m}[\sum_{i=0}^{m}{h_θ(x^{(i)})-y^{(i)})^2}] + \lambda[p\sum_{j=0}^{n}{|θ_{j}|}+(1-p)\sum_{j=0}^{n}{|θ_{j}^2|}]
$$

### 最大似然估计线性回归

对于每一组θ参数作用下，有如下公式：
$$
y^i = θ^Tx^i+ε^i
$$
由于**中心极限定理**，其误差ε是独立同分布的，服从均值为0，方差为某定值$σ^2$的高斯分布，如下：
$$
p(ε^i)=\frac{1}{σ\sqrt{2π}}e^{-\frac{(ε^i)^2}{2σ^2}}
$$
对于一个特定的θ，给定的x在θ的作用下，产生y的可能性概率如下：
$$
p(y^i|x^i;θ)=\frac{1}{σ\sqrt{2π}}e^{-\frac{(y^i-θ^Tx^i)^2}{2σ^2}}
$$
对于所有的样本，根据似然函数如下：
$$
L(θ)=\prod_{i=1}^{m}{p(y^i|x^i,θ)}=\prod_{i=1}^{m}{\frac{1}{σ\sqrt{2π}}e^{-\frac{(y^i-θ^Tx^i)^2}{2σ^2}}}
$$
而目前x与y值是确定的，因此要寻找一个最大的θ值，使得L(θ)的值最大；进行如下操作：两边取对数，之后经过化简可得如下公式：
$$
L(θ)=m\ln\frac{1}{σ\sqrt{2π}}-\frac{1}{σ^2}\frac{1}{2}\sum_{i=0}^{m}{(y^i-θ^Tx^i)^2}
$$
上式中只有一项带有θ值，其他都为常数，因此只需最大化该项，将该项取反即如下所示：
$$
min_θ\frac{1}{2}\sum_{i=0}^{m}{(y^i-θ^Tx^i)^2}
$$
可见结果与最小二乘法的结果是一致的，可以将最小二乘看成是最大似然估计的一个特殊情况。

> 以下两篇博文介绍这个概念介绍的还行：
>
> https://www.jianshu.com/p/fbd736a61927
>
> https://blog.csdn.net/ppn029012/article/details/8908104

## 实际应用

通过sklearn中的API，可以方便的实现线性回归

* 线性回归预测波士顿房价——正规方程

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, SGDRegressor
from sklearn.metrics import mean_squared_error

def liner_regression_model():
    """
    线性回归直接预测波士顿房价
    """
    # 获取数据
    lb = load_boston()

    # 分割数据集为训练集和目标集
    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=0.25)
    # print(x_train, y_train)
    print("test dataset's price:", y_test)

    # 特征值和目标值是都必须进行标准化处理, 实例化两个标准化API
    std_x = StandardScaler()
    x_train = std_x.fit_transform(x_train)
    x_test = std_x.transform(x_test)

    std_y = StandardScaler()
    y_train = std_y.fit_transform(y_train.reshape(-1,1))
    y_test = std_y.fit_transform(y_test.reshape(-1,1))

    # 预测房价价格(正规方程) 普通最小二乘线性回归
    # 拟合模型，得出参数
    lr = LinearRegression()
    lr.fit(x_train, y_train)
    print(lr.coef_)  
    # 表示回归系数w=(w1,w2....)
   	# 预测测试机房子价格
    y_lr_predict = std_y.inverse_transform(lr.predict(x_test))
    # print("lr test dataset's perdict:", y_lr_predict)
    print("lr mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))

```

* 模型保存API

```python
 from sklearn.externals import joblib
    
 # 保存训练好的模型
 # joblib.dump(lr, "./linearRegressionModel.pkl")
 # 用保存好的模型进行预测
 lr = joblib.load("./LinearRegressionModel.pkl")
 # 预测测试机房子价格
 y_lr_predict = std_y.inverse_transform(lr.predict(x_test))
 # print("lr test dataset's perdict:", y_lr_predict)
```

* 回归评估API——均方误差

```python
mean_squared_error(y_true, y_pred)
# 均方误差回归损失
# y_true:真实值
# y_pred:预测值
# return:浮点数结果
```

```python
from sklearn.metrics import mean_squared_error

# 接上述代码
print("lr mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_lr_predict))
```

* 采用梯度下降进行预测

```python
from sklearn.linear_model import SGDRegressor

 # 预测房价价格（梯度下降）
 # 拟合模型，得出参数
 sgd = SGDRegressor()
 sgd.fit(x_train, y_train)
 y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))
 # print("test dataset's perdict:", y_sgd_predict)
 print("sgd mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_sgd_predict))
```

* 岭回归进行预测——L2正则化

```python
sklearn.linear_model.Ridge(alpha=1.0)
# 具有l2正则化的线性最小二乘法

# alpha:正则化力度
# coef_:回归系数
```

```python
from sklearn.linear_model import Ridge

rg = Ridge()
rg.fit(x_train, y_train)
y_rg_predict = std_y.inverse_transform(rg.predict(x_test))
# print("test dataset's perdict:", y_rg_predict)
print("rg mean squared error:",mean_squared_error(std_y.inverse_transform(y_test),y_rg_predict))
```

* Lasso回归——L1正则化
* ElasticNetCV弹性网络

> 从某些教程视频中找到了很多值得参考的代码，都放到了我的GitHub网址下，例如带L1正则化的Lasso回归，同时使用L1和L2正则化的弹性网络算法...
>
> 传送门：https://github.com/zhuChengChao/ML-LinearRegrssion

